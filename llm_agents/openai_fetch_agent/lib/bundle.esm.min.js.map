{"version":3,"file":"bundle.esm.min.js","sources":["../../llm_utils/lib/index.js","../src/openai_fetch_agent.ts"],"sourcesContent":["\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.getMessages = exports.getMergeValue = exports.flatString = void 0;\nconst flatString = (input) => {\n    return Array.isArray(input) ? input.filter((a) => a).join(\"\\n\") : (input ?? \"\");\n};\nexports.flatString = flatString;\nconst getMergeValue = (namedInputs, params, key, values) => {\n    const inputValue = namedInputs[key];\n    const paramsValue = params[key];\n    return inputValue || paramsValue ? [(0, exports.flatString)(inputValue), (0, exports.flatString)(paramsValue)].filter((a) => a).join(\"\\n\") : (0, exports.flatString)(values);\n};\nexports.getMergeValue = getMergeValue;\nconst getMessages = (systemPrompt, messages) => {\n    const messagesCopy = [...(systemPrompt ? [{ role: \"system\", content: systemPrompt }] : []), ...(messages ?? [])];\n    return messagesCopy;\n};\nexports.getMessages = getMessages;\n","import OpenAI from \"openai\";\nimport { AgentFunction, AgentFunctionInfo, sleep } from \"graphai\";\nimport { GraphAILLMInputBase, getMergeValue, getMessages } from \"@graphai/llm_utils\";\n\ntype OpenAIInputs = {\n  model?: string;\n  images?: string[];\n  tools?: OpenAI.ChatCompletionTool[];\n  tool_choice?: OpenAI.ChatCompletionToolChoiceOption;\n  max_tokens?: number;\n  verbose?: boolean;\n  temperature?: number;\n  // baseURL?: string;\n  apiKey?: string;\n  stream?: boolean;\n  messages?: Array<OpenAI.ChatCompletionMessageParam>;\n  forWeb?: boolean;\n  response_format?: any;\n} & GraphAILLMInputBase;\n\nconst convertOpenAIChatCompletion = (response: OpenAI.ChatCompletion, messages: OpenAI.ChatCompletionMessageParam[]) => {\n  const message = response?.choices[0] && response?.choices[0].message ? response?.choices[0].message : null;\n  const text = message && message.content ? message.content : null;\n\n  const functionResponse = message?.tool_calls && message?.tool_calls[0] ? message?.tool_calls[0] : null;\n  // const functionId = message?.tool_calls && message?.tool_calls[0] ? message?.tool_calls[0]?.id : null;\n\n  const tool = functionResponse\n    ? {\n        id: functionResponse.id,\n        name: functionResponse?.function?.name,\n        arguments: (() => {\n          try {\n            return JSON.parse(functionResponse?.function?.arguments);\n          } catch (__e) {\n            return undefined;\n          }\n        })(),\n      }\n    : undefined;\n\n  if (message) {\n    messages.push(message);\n  }\n  return {\n    ...response,\n    text,\n    tool,\n    message,\n    messages,\n  };\n};\n\nexport const openAIFetchAgent: AgentFunction<OpenAIInputs, Record<string, any> | string, string | Array<any>, OpenAIInputs> = async ({\n  filterParams,\n  params,\n  namedInputs,\n}) => {\n  const { verbose, system, images, temperature, tools, tool_choice, max_tokens, /* baseURL, */ stream, apiKey, prompt, messages, response_format } = {\n    ...params,\n    ...namedInputs,\n  };\n\n  const userPrompt = getMergeValue(namedInputs, params, \"mergeablePrompts\", prompt);\n  const systemPrompt = getMergeValue(namedInputs, params, \"mergeableSystem\", system);\n\n  const messagesCopy = getMessages<OpenAI.ChatCompletionMessageParam>(systemPrompt, messages);\n\n  if (!apiKey) {\n    throw new Error(\"OPENAI_API_KEY key is not set in params. params: {apiKey: 'sk-xxx'}\");\n  }\n\n  if (userPrompt) {\n    messagesCopy.push({\n      role: \"user\",\n      content: userPrompt,\n    });\n  }\n  if (images) {\n    const image_url =\n      params.model === \"gpt-4-vision-preview\"\n        ? images[0]\n        : {\n            url: images[0],\n            detail: \"high\",\n          };\n    messagesCopy.push({\n      role: \"user\",\n      content: [\n        {\n          type: \"image_url\",\n          image_url,\n        } as OpenAI.ChatCompletionContentPart,\n      ],\n    });\n  }\n\n  if (verbose) {\n    console.log(messagesCopy);\n  }\n\n  const chatParams = {\n    model: params.model || \"gpt-4o\",\n    messages: messagesCopy as unknown as OpenAI.ChatCompletionMessageParam[],\n    tools,\n    tool_choice,\n    max_tokens,\n    temperature: temperature ?? 0.7,\n    stream: !!stream,\n    response_format,\n  };\n\n  const response = await fetch(\"https://api.openai.com/v1/chat/completions\", {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n      Authorization: `Bearer ${apiKey}`,\n    },\n    body: JSON.stringify(chatParams),\n  });\n\n  if (!stream) {\n    if (response.status === 200) {\n      const result = await response.json();\n      return convertOpenAIChatCompletion(result, messagesCopy);\n    }\n    throw new Error(\"OPENAI API Error\");\n  }\n\n  // streaming\n  const reader = response.body?.getReader();\n\n  if (response.status !== 200 || !reader) {\n    throw new Error(\"Request failed\");\n  }\n\n  const decoder = new TextDecoder(\"utf-8\");\n  let done = false;\n  const buffer = [];\n  let text_buffer = \"\";\n  while (!done) {\n    const { done: readDone, value } = await reader.read();\n    if (readDone) {\n      done = readDone;\n      reader.releaseLock();\n    } else {\n      const text = decoder.decode(value, { stream: true });\n      text_buffer = text + text_buffer;\n      const lines = text_buffer.split(/\\n+/);\n      const next_buff = [];\n      for (const line of lines) {\n        try {\n          const json_text = line.replace(/^data:\\s*/, \"\");\n          if (json_text === \"[DONE]\") {\n            break;\n          } else if (json_text) {\n            const data = JSON.parse(json_text);\n            const token = data.choices[0].delta.content;\n            if (token) {\n              buffer.push(token);\n              if (filterParams && filterParams.streamTokenCallback && token) {\n                filterParams.streamTokenCallback(token);\n              }\n            }\n          }\n        } catch (__error) {\n          next_buff.push(line);\n        }\n      }\n      text_buffer = next_buff.join(\"\\n\");\n    }\n  }\n  return convertOpenAIChatCompletion(\n    {\n      choices: [\n        {\n          message: {\n            role: \"assistant\",\n            content: buffer.join(\"\"),\n            refusal: \"\",\n          },\n        },\n      ],\n    } as any,\n    messagesCopy,\n  );\n};\n\nconst input_sample = \"this is response result\";\nconst result_sample = {\n  object: \"chat.completion\",\n  id: \"chatcmpl-9N7HxXYbwjmdbdiQE94MHoVluQhyt\",\n  choices: [\n    {\n      message: {\n        role: \"assistant\",\n        content: input_sample,\n      },\n      finish_reason: \"stop\",\n      index: 0,\n      logprobs: null,\n    },\n  ],\n  created: 1715296589,\n  model: \"gpt-3.5-turbo-0125\",\n};\n\nexport const openAIMockAgent: AgentFunction<\n  {\n    model?: string;\n    query?: string;\n    system?: string;\n    verbose?: boolean;\n    temperature?: number;\n  },\n  Record<string, any> | string,\n  string | Array<any>\n> = async ({ filterParams }) => {\n  for await (const token of input_sample.split(\"\")) {\n    if (filterParams && filterParams.streamTokenCallback && token) {\n      await sleep(100);\n      filterParams.streamTokenCallback(token);\n    }\n  }\n\n  return result_sample;\n};\nconst openAIFetchAgentInfo: AgentFunctionInfo = {\n  name: \"openAIFetchAgent\",\n  agent: openAIFetchAgent,\n  mock: openAIMockAgent,\n  inputs: {\n    type: \"object\",\n    properties: {\n      model: { type: \"string\" },\n      system: { type: \"string\" },\n      tools: { type: \"object\" },\n      tool_choice: {\n        anyOf: [{ type: \"array\" }, { type: \"object\" }],\n      },\n      max_tokens: { type: \"number\" },\n      verbose: { type: \"boolean\" },\n      temperature: { type: \"number\" },\n      // baseURL: { type: \"string\" },\n      apiKey: {\n        anyOf: [{ type: \"string\" }, { type: \"object\" }],\n      },\n      stream: { type: \"boolean\" },\n      prompt: {\n        type: \"string\",\n        description: \"query string\",\n      },\n      messages: {\n        anyOf: [{ type: \"string\" }, { type: \"object\" }, { type: \"array\" }],\n        description: \"chat messages\",\n      },\n    },\n  },\n  output: {\n    type: \"object\",\n    properties: {\n      id: {\n        type: \"string\",\n      },\n      object: {\n        type: \"string\",\n      },\n      created: {\n        type: \"integer\",\n      },\n      model: {\n        type: \"string\",\n      },\n      choices: {\n        type: \"array\",\n        items: [\n          {\n            type: \"object\",\n            properties: {\n              index: {\n                type: \"integer\",\n              },\n              message: {\n                type: \"array\",\n                items: [\n                  {\n                    type: \"object\",\n                    properties: {\n                      content: {\n                        type: \"string\",\n                      },\n                      role: {\n                        type: \"string\",\n                      },\n                    },\n                    required: [\"content\", \"role\"],\n                  },\n                ],\n              },\n            },\n            required: [\"index\", \"message\", \"logprobs\", \"finish_reason\"],\n          },\n        ],\n      },\n      usage: {\n        type: \"object\",\n        properties: {\n          prompt_tokens: {\n            type: \"integer\",\n          },\n          completion_tokens: {\n            type: \"integer\",\n          },\n          total_tokens: {\n            type: \"integer\",\n          },\n        },\n        required: [\"prompt_tokens\", \"completion_tokens\", \"total_tokens\"],\n      },\n      text: {\n        type: \"string\",\n      },\n      tool: {\n        arguments: {\n          type: \"object\",\n        },\n        name: {\n          type: \"string\",\n        },\n      },\n      message: {\n        type: \"object\",\n        properties: {\n          content: {\n            type: \"string\",\n          },\n          role: {\n            type: \"string\",\n          },\n        },\n        required: [\"content\", \"role\"],\n      },\n    },\n    required: [\"id\", \"object\", \"created\", \"model\", \"choices\", \"usage\"],\n  },\n  params: {\n    type: \"object\",\n    properties: {\n      model: { type: \"string\" },\n      system: { type: \"string\" },\n      tools: { type: \"object\" },\n      tool_choice: { anyOf: [{ type: \"array\" }, { type: \"object\" }] },\n      max_tokens: { type: \"number\" },\n      verbose: { type: \"boolean\" },\n      temperature: { type: \"number\" },\n      // baseURL: { type: \"string\" },\n      apiKey: { anyOf: [{ type: \"string\" }, { type: \"object\" }] },\n      stream: { type: \"boolean\" },\n      prompt: { type: \"string\", description: \"query string\" },\n      messages: { anyOf: [{ type: \"string\" }, { type: \"object\" }, { type: \"array\" }], description: \"chat messages\" },\n    },\n  },\n  outputFormat: {\n    llmResponse: {\n      key: \"choices.$0.message.content\",\n      type: \"string\",\n    },\n  },\n  samples: [\n    {\n      inputs: { prompt: input_sample },\n      params: {},\n      result: result_sample,\n    },\n  ],\n  description: \"OpenAI Fetch Agent\",\n  category: [\"llm\"],\n  author: \"Receptron team\",\n  repository: \"https://github.com/receptron/graphai\",\n  license: \"MIT\",\n  stream: true,\n  npms: [\"openai\"],\n};\n\nexport default openAIFetchAgentInfo;\n"],"names":["Object","defineProperty","exports","value","getMessages","getMergeValue","flatString","input","Array","isArray","filter","a","join","namedInputs","params","key","values","inputValue","paramsValue","systemPrompt","messages","role","content","convertOpenAIChatCompletion","response","message","choices","text","functionResponse","tool_calls","tool","id","name","function","arguments","JSON","parse","__e","undefined","push","input_sample","result_sample","object","finish_reason","index","logprobs","created","model","openAIFetchAgentInfo","agent","async","filterParams","verbose","system","images","temperature","tools","tool_choice","max_tokens","stream","apiKey","prompt","response_format","userPrompt","messagesCopy","Error","image_url","url","detail","type","console","log","chatParams","fetch","method","headers","Authorization","body","stringify","status","result","json","reader","getReader","decoder","TextDecoder","done","buffer","text_buffer","readDone","read","releaseLock","decode","lines","split","next_buff","line","json_text","replace","token","delta","streamTokenCallback","__error","refusal","mock","sleep","inputs","properties","anyOf","description","output","items","required","usage","prompt_tokens","completion_tokens","total_tokens","outputFormat","llmResponse","samples","category","author","repository","license","npms"],"mappings":"gEACAA,OAAOC,eAAcC,EAAU,aAAc,CAAEC,OAAO,IACtDD,EAAAE,YAAsBF,EAAwBG,cAAAH,EAAAI,gBAAqB,EAInEJ,EAAAI,WAHoBC,GACTC,MAAMC,QAAQF,GAASA,EAAMG,QAAQC,GAAMA,IAAGC,KAAK,MAASL,GAAS,GAQhFL,EAAAG,cALsB,CAACQ,EAAaC,EAAQC,EAAKC,KAC7C,MAAMC,EAAaJ,EAAYE,GACzBG,EAAcJ,EAAOC,GAC3B,OAAOE,GAAcC,EAAc,EAAC,EAAIhB,EAAQI,YAAYW,IAAa,EAAIf,EAAQI,YAAYY,IAAcR,QAAQC,GAAMA,IAAGC,KAAK,OAAQ,EAAIV,EAAQI,YAAYU,EAAO,OAOhLd,EAAAE,YAJoB,CAACe,EAAcC,IACV,IAAKD,EAAe,CAAC,CAAEE,KAAM,SAAUC,QAASH,IAAkB,MAASC,GAAY,SCMhH,MAAMG,EAA8B,CAACC,EAAiCJ,KACpE,MAAMK,EAAUD,GAAUE,QAAQ,IAAMF,GAAUE,QAAQ,GAAGD,QAAUD,GAAUE,QAAQ,GAAGD,QAAU,KAChGE,EAAOF,GAAWA,EAAQH,QAAUG,EAAQH,QAAU,KAEtDM,EAAmBH,GAASI,YAAcJ,GAASI,WAAW,GAAKJ,GAASI,WAAW,GAAK,KAG5FC,EAAOF,EACT,CACEG,GAAIH,EAAiBG,GACrBC,KAAMJ,GAAkBK,UAAUD,KAClCE,UAAW,MACT,IACE,OAAOC,KAAKC,MAAMR,GAAkBK,UAAUC,WAC9C,MAAOG,GACP,OAEH,EANU,SAQbC,EAKJ,OAHIb,GACFL,EAASmB,KAAKd,GAET,IACFD,EACHG,OACAG,OACAL,UACAL,WACD,EA0IGoB,EAAe,0BACfC,EAAgB,CACpBC,OAAQ,kBACRX,GAAI,yCACJL,QAAS,CACP,CACED,QAAS,CACPJ,KAAM,YACNC,QAASkB,GAEXG,cAAe,OACfC,MAAO,EACPC,SAAU,OAGdC,QAAS,WACTC,MAAO,sBAuBHC,EAA0C,CAC9ChB,KAAM,mBACNiB,MAhL4HC,OAC5HC,eACArC,SACAD,kBAEA,MAAMuC,QAAEA,EAAOC,OAAEA,EAAMC,OAAEA,EAAMC,YAAEA,EAAWC,MAAEA,EAAKC,YAAEA,EAAWC,WAAEA,EAAUC,OAAiBA,EAAMC,OAAEA,EAAMC,OAAEA,EAAMzC,SAAEA,EAAQ0C,gBAAEA,GAAoB,IAC9IhD,KACAD,GAGCkD,EAAa1D,EAAAA,cAAcQ,EAAaC,EAAQ,mBAAoB+C,GACpE1C,EAAed,EAAAA,cAAcQ,EAAaC,EAAQ,kBAAmBuC,GAErEW,EAAe5D,EAAAA,YAA+Ce,EAAcC,GAElF,IAAKwC,EACH,MAAM,IAAIK,MAAM,uEASlB,GANIF,GACFC,EAAazB,KAAK,CAChBlB,KAAM,OACNC,QAASyC,IAGTT,EAAQ,CACV,MAAMY,EACa,yBAAjBpD,EAAOiC,MACHO,EAAO,GACP,CACEa,IAAKb,EAAO,GACZc,OAAQ,QAEhBJ,EAAazB,KAAK,CAChBlB,KAAM,OACNC,QAAS,CACP,CACE+C,KAAM,YACNH,gBAMJd,GACFkB,QAAQC,IAAIP,GAGd,MAAMQ,EAAa,CACjBzB,MAAOjC,EAAOiC,OAAS,SACvB3B,SAAU4C,EACVR,QACAC,cACAC,aACAH,YAAaA,GAAe,GAC5BI,SAAUA,EACVG,mBAGItC,QAAiBiD,MAAM,6CAA8C,CACzEC,OAAQ,OACRC,QAAS,CACP,eAAgB,mBAChBC,cAAe,UAAUhB,KAE3BiB,KAAM1C,KAAK2C,UAAUN,KAGvB,IAAKb,EAAQ,CACX,GAAwB,MAApBnC,EAASuD,OAAgB,CAC3B,MAAMC,QAAexD,EAASyD,OAC9B,OAAO1D,EAA4ByD,EAAQhB,GAE7C,MAAM,IAAIC,MAAM,oBAIlB,MAAMiB,EAAS1D,EAASqD,MAAMM,YAE9B,GAAwB,MAApB3D,EAASuD,SAAmBG,EAC9B,MAAM,IAAIjB,MAAM,kBAGlB,MAAMmB,EAAU,IAAIC,YAAY,SAChC,IAAIC,GAAO,EACX,MAAMC,EAAS,GACf,IAAIC,EAAc,GAClB,MAAQF,GAAM,CACZ,MAAQA,KAAMG,EAAQtF,MAAEA,SAAgB+E,EAAOQ,OAC/C,GAAID,EACFH,EAAOG,EACPP,EAAOS,kBACF,CAELH,EADaJ,EAAQQ,OAAOzF,EAAO,CAAEwD,QAAQ,IACxB6B,EACrB,MAAMK,EAAQL,EAAYM,MAAM,OAC1BC,EAAY,GAClB,IAAK,MAAMC,KAAQH,EACjB,IACE,MAAMI,EAAYD,EAAKE,QAAQ,YAAa,IAC5C,GAAkB,WAAdD,EACF,MACK,GAAIA,EAAW,CACpB,MACME,EADOhE,KAAKC,MAAM6D,GACLvE,QAAQ,GAAG0E,MAAM9E,QAChC6E,IACFZ,EAAOhD,KAAK4D,GACRhD,GAAgBA,EAAakD,qBAAuBF,GACtDhD,EAAakD,oBAAoBF,KAIvC,MAAOG,GACPP,EAAUxD,KAAKyD,GAGnBR,EAAcO,EAAUnF,KAAK,OAGjC,OAAOW,EACL,CACEG,QAAS,CACP,CACED,QAAS,CACPJ,KAAM,YACNC,QAASiE,EAAO3E,KAAK,IACrB2F,QAAS,OAKjBvC,EACD,EA6CDwC,KAbEtD,OAASC,mBACX,UAAW,MAAMgD,KAAS3D,EAAasD,MAAM,IACvC3C,GAAgBA,EAAakD,qBAAuBF,UAChDM,EAAM,KACZtD,EAAakD,oBAAoBF,IAIrC,OAAO1D,CAAa,EAMpBiE,OAAQ,CACNrC,KAAM,SACNsC,WAAY,CACV5D,MAAO,CAAEsB,KAAM,UACfhB,OAAQ,CAAEgB,KAAM,UAChBb,MAAO,CAAEa,KAAM,UACfZ,YAAa,CACXmD,MAAO,CAAC,CAAEvC,KAAM,SAAW,CAAEA,KAAM,YAErCX,WAAY,CAAEW,KAAM,UACpBjB,QAAS,CAAEiB,KAAM,WACjBd,YAAa,CAAEc,KAAM,UAErBT,OAAQ,CACNgD,MAAO,CAAC,CAAEvC,KAAM,UAAY,CAAEA,KAAM,YAEtCV,OAAQ,CAAEU,KAAM,WAChBR,OAAQ,CACNQ,KAAM,SACNwC,YAAa,gBAEfzF,SAAU,CACRwF,MAAO,CAAC,CAAEvC,KAAM,UAAY,CAAEA,KAAM,UAAY,CAAEA,KAAM,UACxDwC,YAAa,mBAInBC,OAAQ,CACNzC,KAAM,SACNsC,WAAY,CACV5E,GAAI,CACFsC,KAAM,UAER3B,OAAQ,CACN2B,KAAM,UAERvB,QAAS,CACPuB,KAAM,WAERtB,MAAO,CACLsB,KAAM,UAER3C,QAAS,CACP2C,KAAM,QACN0C,MAAO,CACL,CACE1C,KAAM,SACNsC,WAAY,CACV/D,MAAO,CACLyB,KAAM,WAER5C,QAAS,CACP4C,KAAM,QACN0C,MAAO,CACL,CACE1C,KAAM,SACNsC,WAAY,CACVrF,QAAS,CACP+C,KAAM,UAERhD,KAAM,CACJgD,KAAM,WAGV2C,SAAU,CAAC,UAAW,YAK9BA,SAAU,CAAC,QAAS,UAAW,WAAY,oBAIjDC,MAAO,CACL5C,KAAM,SACNsC,WAAY,CACVO,cAAe,CACb7C,KAAM,WAER8C,kBAAmB,CACjB9C,KAAM,WAER+C,aAAc,CACZ/C,KAAM,YAGV2C,SAAU,CAAC,gBAAiB,oBAAqB,iBAEnDrF,KAAM,CACJ0C,KAAM,UAERvC,KAAM,CACJI,UAAW,CACTmC,KAAM,UAERrC,KAAM,CACJqC,KAAM,WAGV5C,QAAS,CACP4C,KAAM,SACNsC,WAAY,CACVrF,QAAS,CACP+C,KAAM,UAERhD,KAAM,CACJgD,KAAM,WAGV2C,SAAU,CAAC,UAAW,UAG1BA,SAAU,CAAC,KAAM,SAAU,UAAW,QAAS,UAAW,UAE5DlG,OAAQ,CACNuD,KAAM,SACNsC,WAAY,CACV5D,MAAO,CAAEsB,KAAM,UACfhB,OAAQ,CAAEgB,KAAM,UAChBb,MAAO,CAAEa,KAAM,UACfZ,YAAa,CAAEmD,MAAO,CAAC,CAAEvC,KAAM,SAAW,CAAEA,KAAM,YAClDX,WAAY,CAAEW,KAAM,UACpBjB,QAAS,CAAEiB,KAAM,WACjBd,YAAa,CAAEc,KAAM,UAErBT,OAAQ,CAAEgD,MAAO,CAAC,CAAEvC,KAAM,UAAY,CAAEA,KAAM,YAC9CV,OAAQ,CAAEU,KAAM,WAChBR,OAAQ,CAAEQ,KAAM,SAAUwC,YAAa,gBACvCzF,SAAU,CAAEwF,MAAO,CAAC,CAAEvC,KAAM,UAAY,CAAEA,KAAM,UAAY,CAAEA,KAAM,UAAYwC,YAAa,mBAGjGQ,aAAc,CACZC,YAAa,CACXvG,IAAK,6BACLsD,KAAM,WAGVkD,QAAS,CACP,CACEb,OAAQ,CAAE7C,OAAQrB,GAClB1B,OAAQ,CAAE,EACVkE,OAAQvC,IAGZoE,YAAa,qBACbW,SAAU,CAAC,OACXC,OAAQ,iBACRC,WAAY,uCACZC,QAAS,MACThE,QAAQ,EACRiE,KAAM,CAAC"}