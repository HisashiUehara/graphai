{"version":3,"file":"bundle.cjs.min.js","sources":["../../llm_utils/lib/index.js","../src/openai_fetch_agent.ts"],"sourcesContent":["\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.getMessages = exports.getMergeValue = exports.flatString = void 0;\nconst flatString = (input) => {\n    return Array.isArray(input) ? input.filter((a) => a).join(\"\\n\") : (input ?? \"\");\n};\nexports.flatString = flatString;\nconst getMergeValue = (namedInputs, params, key, values) => {\n    const inputValue = namedInputs[key];\n    const paramsValue = params[key];\n    return inputValue || paramsValue ? [(0, exports.flatString)(inputValue), (0, exports.flatString)(paramsValue)].filter((a) => a).join(\"\\n\") : (0, exports.flatString)(values);\n};\nexports.getMergeValue = getMergeValue;\nconst getMessages = (systemPrompt, messages) => {\n    const messagesCopy = [...(systemPrompt ? [{ role: \"system\", content: systemPrompt }] : []), ...(messages ?? [])];\n    return messagesCopy;\n};\nexports.getMessages = getMessages;\n","import OpenAI from \"openai\";\nimport { AgentFunction, AgentFunctionInfo, sleep } from \"graphai\";\nimport { GraphAILLMInputBase, getMergeValue, getMessages } from \"@graphai/llm_utils\";\n\ntype OpenAIInputs = {\n  model?: string;\n  images?: string[];\n  tools?: OpenAI.ChatCompletionTool[];\n  tool_choice?: OpenAI.ChatCompletionToolChoiceOption;\n  max_tokens?: number;\n  verbose?: boolean;\n  temperature?: number;\n  baseURL?: string;\n  apiKey?: string;\n  stream?: boolean;\n  messages?: Array<OpenAI.ChatCompletionMessageParam>;\n  forWeb?: boolean;\n  response_format?: any;\n} & GraphAILLMInputBase;\n\nconst convertOpenAIChatCompletion = (response: OpenAI.ChatCompletion, messages: OpenAI.ChatCompletionMessageParam[]) => {\n  const message = response?.choices[0] && response?.choices[0].message ? response?.choices[0].message : null;\n  const text = message && message.content ? message.content : null;\n\n  const functionResponse = message?.tool_calls && message?.tool_calls[0] ? message?.tool_calls[0] : null;\n  // const functionId = message?.tool_calls && message?.tool_calls[0] ? message?.tool_calls[0]?.id : null;\n\n  const tool = functionResponse\n    ? {\n        id: functionResponse.id,\n        name: functionResponse?.function?.name,\n        arguments: (() => {\n          try {\n            return JSON.parse(functionResponse?.function?.arguments);\n          } catch (__e) {\n            return undefined;\n          }\n        })(),\n      }\n    : undefined;\n\n  if (message) {\n    messages.push(message);\n  }\n  return {\n    ...response,\n    text,\n    tool,\n    message,\n    messages,\n  };\n};\n\nexport const openAIFetchAgent: AgentFunction<OpenAIInputs, Record<string, any> | string, string | Array<any>, OpenAIInputs> = async ({\n  filterParams,\n  params,\n  namedInputs,\n}) => {\n  const { verbose, system, images, temperature, tools, tool_choice, max_tokens, baseURL, apiKey, stream, prompt, messages, forWeb, response_format } = {\n    ...params,\n    ...namedInputs,\n  };\n\n  const userPrompt = getMergeValue(namedInputs, params, \"mergeablePrompts\", prompt);\n  const systemPrompt = getMergeValue(namedInputs, params, \"mergeableSystem\", system);\n\n  const messagesCopy = getMessages<OpenAI.ChatCompletionMessageParam>(systemPrompt, messages);\n\n  if (!apiKey) {\n    throw new Error(\"OPENAI_API_KEY key is not set in params. params: {apiKey: 'sk-xxx'}\");\n  }\n\n  if (userPrompt) {\n    messagesCopy.push({\n      role: \"user\",\n      content: userPrompt,\n    });\n  }\n  if (images) {\n    const image_url =\n      params.model === \"gpt-4-vision-preview\"\n        ? images[0]\n        : {\n            url: images[0],\n            detail: \"high\",\n          };\n    messagesCopy.push({\n      role: \"user\",\n      content: [\n        {\n          type: \"image_url\",\n          image_url,\n        } as OpenAI.ChatCompletionContentPart,\n      ],\n    });\n  }\n\n  if (verbose) {\n    console.log(messagesCopy);\n  }\n\n  const chatParams = {\n    model: params.model || \"gpt-4o\",\n    messages: messagesCopy as unknown as OpenAI.ChatCompletionMessageParam[],\n    tools,\n    tool_choice,\n    max_tokens,\n    temperature: temperature ?? 0.7,\n    response_format,\n  };\n\n  const response = await fetch(\"https://api.openai.com/v1/chat/completions\", {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n      \"Authorization\": `Bearer ${apiKey}`\n    },\n    body: JSON.stringify(chatParams)\n  });\n\n  if (response.status === 200) {\n    const result = await response.json();\n    return convertOpenAIChatCompletion(result, messagesCopy);\n  }\n  throw new Error(\"OPENAI API Error\");\n};\n\nconst input_sample = \"this is response result\";\nconst result_sample = {\n  object: \"chat.completion\",\n  id: \"chatcmpl-9N7HxXYbwjmdbdiQE94MHoVluQhyt\",\n  choices: [\n    {\n      message: {\n        role: \"assistant\",\n        content: input_sample,\n      },\n      finish_reason: \"stop\",\n      index: 0,\n      logprobs: null,\n    },\n  ],\n  created: 1715296589,\n  model: \"gpt-3.5-turbo-0125\",\n};\n\nexport const openAIMockAgent: AgentFunction<\n  {\n    model?: string;\n    query?: string;\n    system?: string;\n    verbose?: boolean;\n    temperature?: number;\n  },\n  Record<string, any> | string,\n  string | Array<any>\n> = async ({ filterParams }) => {\n  for await (const token of input_sample.split(\"\")) {\n    if (filterParams && filterParams.streamTokenCallback && token) {\n      await sleep(100);\n      filterParams.streamTokenCallback(token);\n    }\n  }\n\n  return result_sample;\n};\nconst openAIFetchAgentInfo: AgentFunctionInfo = {\n  name: \"openAIFetchAgent\",\n  agent: openAIFetchAgent,\n  mock: openAIMockAgent,\n  inputs: {\n    type: \"object\",\n    properties: {\n      model: { type: \"string\" },\n      system: { type: \"string\" },\n      tools: { type: \"object\" },\n      tool_choice: {\n        anyOf: [{ type: \"array\" }, { type: \"object\" }],\n      },\n      max_tokens: { type: \"number\" },\n      verbose: { type: \"boolean\" },\n      temperature: { type: \"number\" },\n      baseURL: { type: \"string\" },\n      apiKey: {\n        anyOf: [{ type: \"string\" }, { type: \"object\" }],\n      },\n      stream: { type: \"boolean\" },\n      prompt: {\n        type: \"string\",\n        description: \"query string\",\n      },\n      messages: {\n        anyOf: [{ type: \"string\" }, { type: \"object\" }, { type: \"array\" }],\n        description: \"chat messages\",\n      },\n    },\n  },\n  output: {\n    type: \"object\",\n    properties: {\n      id: {\n        type: \"string\",\n      },\n      object: {\n        type: \"string\",\n      },\n      created: {\n        type: \"integer\",\n      },\n      model: {\n        type: \"string\",\n      },\n      choices: {\n        type: \"array\",\n        items: [\n          {\n            type: \"object\",\n            properties: {\n              index: {\n                type: \"integer\",\n              },\n              message: {\n                type: \"array\",\n                items: [\n                  {\n                    type: \"object\",\n                    properties: {\n                      content: {\n                        type: \"string\",\n                      },\n                      role: {\n                        type: \"string\",\n                      },\n                    },\n                    required: [\"content\", \"role\"],\n                  },\n                ],\n              },\n            },\n            required: [\"index\", \"message\", \"logprobs\", \"finish_reason\"],\n          },\n        ],\n      },\n      usage: {\n        type: \"object\",\n        properties: {\n          prompt_tokens: {\n            type: \"integer\",\n          },\n          completion_tokens: {\n            type: \"integer\",\n          },\n          total_tokens: {\n            type: \"integer\",\n          },\n        },\n        required: [\"prompt_tokens\", \"completion_tokens\", \"total_tokens\"],\n      },\n      text: {\n        type: \"string\",\n      },\n      tool: {\n        arguments: {\n          type: \"object\",\n        },\n        name: {\n          type: \"string\",\n        },\n      },\n      message: {\n        type: \"object\",\n        properties: {\n          content: {\n            type: \"string\",\n          },\n          role: {\n            type: \"string\",\n          },\n        },\n        required: [\"content\", \"role\"],\n      },\n    },\n    required: [\"id\", \"object\", \"created\", \"model\", \"choices\", \"usage\"],\n  },\n  params: {\n    type: \"object\",\n    properties: {\n      model: { type: \"string\" },\n      system: { type: \"string\" },\n      tools: { type: \"object\" },\n      tool_choice: { anyOf: [{ type: \"array\" }, { type: \"object\" }] },\n      max_tokens: { type: \"number\" },\n      verbose: { type: \"boolean\" },\n      temperature: { type: \"number\" },\n      baseURL: { type: \"string\" },\n      apiKey: { anyOf: [{ type: \"string\" }, { type: \"object\" }] },\n      stream: { type: \"boolean\" },\n      prompt: { type: \"string\", description: \"query string\" },\n      messages: { anyOf: [{ type: \"string\" }, { type: \"object\" }, { type: \"array\" }], description: \"chat messages\" },\n    },\n  },\n  outputFormat: {\n    llmResponse: {\n      key: \"choices.$0.message.content\",\n      type: \"string\",\n    },\n  },\n  samples: [\n    {\n      inputs: { prompt: input_sample },\n      params: {},\n      result: result_sample,\n    },\n  ],\n  description: \"OpenAI Fetch Agent\",\n  category: [\"llm\"],\n  author: \"Receptron team\",\n  repository: \"https://github.com/receptron/graphai\",\n  license: \"MIT\",\n  stream: true,\n  npms: [\"openai\"],\n};\n\nexport default openAIFetchAgentInfo;\n"],"names":["Object","defineProperty","exports","value","getMessages","getMergeValue","flatString","input","Array","isArray","filter","a","join","namedInputs","params","key","values","inputValue","paramsValue","systemPrompt","messages","role","content","input_sample","result_sample","object","id","choices","message","finish_reason","index","logprobs","created","model","openAIFetchAgentInfo","name","agent","async","filterParams","verbose","system","images","temperature","tools","tool_choice","max_tokens","baseURL","apiKey","stream","prompt","forWeb","response_format","userPrompt","messagesCopy","Error","push","image_url","url","detail","type","console","log","chatParams","response","fetch","method","headers","Authorization","body","JSON","stringify","status","text","functionResponse","tool_calls","tool","function","arguments","parse","__e","undefined","convertOpenAIChatCompletion","json","mock","token","split","streamTokenCallback","sleep","inputs","properties","anyOf","description","output","items","required","usage","prompt_tokens","completion_tokens","total_tokens","outputFormat","llmResponse","samples","result","category","author","repository","license","npms"],"mappings":"wEACAA,OAAOC,eAAcC,EAAU,aAAc,CAAEC,OAAO,IACtDD,EAAAE,YAAsBF,EAAwBG,cAAAH,EAAAI,gBAAqB,EAInEJ,EAAAI,WAHoBC,GACTC,MAAMC,QAAQF,GAASA,EAAMG,QAAQC,GAAMA,IAAGC,KAAK,MAASL,GAAS,GAQhFL,EAAAG,cALsB,CAACQ,EAAaC,EAAQC,EAAKC,KAC7C,MAAMC,EAAaJ,EAAYE,GACzBG,EAAcJ,EAAOC,GAC3B,OAAOE,GAAcC,EAAc,EAAC,EAAIhB,EAAQI,YAAYW,IAAa,EAAIf,EAAQI,YAAYY,IAAcR,QAAQC,GAAMA,IAAGC,KAAK,OAAQ,EAAIV,EAAQI,YAAYU,EAAO,EAOhLd,EAAAE,YAJoB,CAACe,EAAcC,IACV,IAAKD,EAAe,CAAC,CAAEE,KAAM,SAAUC,QAASH,IAAkB,MAASC,GAAY,YCMhH,MA2GMG,EAAe,0BACfC,EAAgB,CACpBC,OAAQ,kBACRC,GAAI,yCACJC,QAAS,CACP,CACEC,QAAS,CACPP,KAAM,YACNC,QAASC,GAEXM,cAAe,OACfC,MAAO,EACPC,SAAU,OAGdC,QAAS,WACTC,MAAO,sBAuBHC,EAA0C,CAC9CC,KAAM,mBACNC,MAnH4HC,OAC5HC,eACAxB,SACAD,kBAEA,MAAM0B,QAAEA,EAAOC,OAAEA,EAAMC,OAAEA,EAAMC,YAAEA,EAAWC,MAAEA,EAAKC,YAAEA,EAAWC,WAAEA,EAAUC,QAAEA,EAAOC,OAAEA,EAAMC,OAAEA,EAAMC,OAAEA,EAAM7B,SAAEA,EAAQ8B,OAAEA,EAAMC,gBAAEA,GAAoB,IAChJrC,KACAD,GAGCuC,EAAa/C,EAAAA,cAAcQ,EAAaC,EAAQ,mBAAoBmC,GACpE9B,EAAed,EAAAA,cAAcQ,EAAaC,EAAQ,kBAAmB0B,GAErEa,EAAejD,EAAAA,YAA+Ce,EAAcC,GAElF,IAAK2B,EACH,MAAM,IAAIO,MAAM,uEASlB,GANIF,GACFC,EAAaE,KAAK,CAChBlC,KAAM,OACNC,QAAS8B,IAGTX,EAAQ,CACV,MAAMe,EACa,yBAAjB1C,EAAOmB,MACHQ,EAAO,GACP,CACEgB,IAAKhB,EAAO,GACZiB,OAAQ,QAEhBL,EAAaE,KAAK,CAChBlC,KAAM,OACNC,QAAS,CACP,CACEqC,KAAM,YACNH,gBAMJjB,GACFqB,QAAQC,IAAIR,GAGd,MAAMS,EAAa,CACjB7B,MAAOnB,EAAOmB,OAAS,SACvBb,SAAUiC,EACVV,QACAC,cACAC,aACAH,YAAaA,GAAe,GAC5BS,mBAGIY,QAAiBC,MAAM,6CAA8C,CACzEC,OAAQ,OACRC,QAAS,CACP,eAAgB,mBAChBC,cAAiB,UAAUpB,KAE7BqB,KAAMC,KAAKC,UAAUR,KAGvB,GAAwB,MAApBC,EAASQ,OAAgB,CAE3B,MAtGgC,EAACR,EAAiC3C,KACpE,MAAMQ,EAAUmC,GAAUpC,QAAQ,IAAMoC,GAAUpC,QAAQ,GAAGC,QAAUmC,GAAUpC,QAAQ,GAAGC,QAAU,KAChG4C,EAAO5C,GAAWA,EAAQN,QAAUM,EAAQN,QAAU,KAEtDmD,EAAmB7C,GAAS8C,YAAc9C,GAAS8C,WAAW,GAAK9C,GAAS8C,WAAW,GAAK,KAG5FC,EAAOF,EACT,CACE/C,GAAI+C,EAAiB/C,GACrBS,KAAMsC,GAAkBG,UAAUzC,KAClC0C,UAAW,MACT,IACE,OAAOR,KAAKS,MAAML,GAAkBG,UAAUC,WAC9C,MAAOE,GACP,OAEH,EANU,SAQbC,EAKJ,OAHIpD,GACFR,EAASmC,KAAK3B,GAET,IACFmC,EACHS,OACAG,OACA/C,UACAR,WACD,EAwEQ6D,OADclB,EAASmB,OACa7B,GAE7C,MAAM,IAAIC,MAAM,mBAAmB,EA6CnC6B,KAbE9C,OAASC,mBACX,UAAW,MAAM8C,KAAS7D,EAAa8D,MAAM,IACvC/C,GAAgBA,EAAagD,qBAAuBF,UAChDG,EAAAA,MAAM,KACZjD,EAAagD,oBAAoBF,IAIrC,OAAO5D,CAAa,EAMpBgE,OAAQ,CACN7B,KAAM,SACN8B,WAAY,CACVxD,MAAO,CAAE0B,KAAM,UACfnB,OAAQ,CAAEmB,KAAM,UAChBhB,MAAO,CAAEgB,KAAM,UACff,YAAa,CACX8C,MAAO,CAAC,CAAE/B,KAAM,SAAW,CAAEA,KAAM,YAErCd,WAAY,CAAEc,KAAM,UACpBpB,QAAS,CAAEoB,KAAM,WACjBjB,YAAa,CAAEiB,KAAM,UACrBb,QAAS,CAAEa,KAAM,UACjBZ,OAAQ,CACN2C,MAAO,CAAC,CAAE/B,KAAM,UAAY,CAAEA,KAAM,YAEtCX,OAAQ,CAAEW,KAAM,WAChBV,OAAQ,CACNU,KAAM,SACNgC,YAAa,gBAEfvE,SAAU,CACRsE,MAAO,CAAC,CAAE/B,KAAM,UAAY,CAAEA,KAAM,UAAY,CAAEA,KAAM,UACxDgC,YAAa,mBAInBC,OAAQ,CACNjC,KAAM,SACN8B,WAAY,CACV/D,GAAI,CACFiC,KAAM,UAERlC,OAAQ,CACNkC,KAAM,UAER3B,QAAS,CACP2B,KAAM,WAER1B,MAAO,CACL0B,KAAM,UAERhC,QAAS,CACPgC,KAAM,QACNkC,MAAO,CACL,CACElC,KAAM,SACN8B,WAAY,CACV3D,MAAO,CACL6B,KAAM,WAER/B,QAAS,CACP+B,KAAM,QACNkC,MAAO,CACL,CACElC,KAAM,SACN8B,WAAY,CACVnE,QAAS,CACPqC,KAAM,UAERtC,KAAM,CACJsC,KAAM,WAGVmC,SAAU,CAAC,UAAW,YAK9BA,SAAU,CAAC,QAAS,UAAW,WAAY,oBAIjDC,MAAO,CACLpC,KAAM,SACN8B,WAAY,CACVO,cAAe,CACbrC,KAAM,WAERsC,kBAAmB,CACjBtC,KAAM,WAERuC,aAAc,CACZvC,KAAM,YAGVmC,SAAU,CAAC,gBAAiB,oBAAqB,iBAEnDtB,KAAM,CACJb,KAAM,UAERgB,KAAM,CACJE,UAAW,CACTlB,KAAM,UAERxB,KAAM,CACJwB,KAAM,WAGV/B,QAAS,CACP+B,KAAM,SACN8B,WAAY,CACVnE,QAAS,CACPqC,KAAM,UAERtC,KAAM,CACJsC,KAAM,WAGVmC,SAAU,CAAC,UAAW,UAG1BA,SAAU,CAAC,KAAM,SAAU,UAAW,QAAS,UAAW,UAE5DhF,OAAQ,CACN6C,KAAM,SACN8B,WAAY,CACVxD,MAAO,CAAE0B,KAAM,UACfnB,OAAQ,CAAEmB,KAAM,UAChBhB,MAAO,CAAEgB,KAAM,UACff,YAAa,CAAE8C,MAAO,CAAC,CAAE/B,KAAM,SAAW,CAAEA,KAAM,YAClDd,WAAY,CAAEc,KAAM,UACpBpB,QAAS,CAAEoB,KAAM,WACjBjB,YAAa,CAAEiB,KAAM,UACrBb,QAAS,CAAEa,KAAM,UACjBZ,OAAQ,CAAE2C,MAAO,CAAC,CAAE/B,KAAM,UAAY,CAAEA,KAAM,YAC9CX,OAAQ,CAAEW,KAAM,WAChBV,OAAQ,CAAEU,KAAM,SAAUgC,YAAa,gBACvCvE,SAAU,CAAEsE,MAAO,CAAC,CAAE/B,KAAM,UAAY,CAAEA,KAAM,UAAY,CAAEA,KAAM,UAAYgC,YAAa,mBAGjGQ,aAAc,CACZC,YAAa,CACXrF,IAAK,6BACL4C,KAAM,WAGV0C,QAAS,CACP,CACEb,OAAQ,CAAEvC,OAAQ1B,GAClBT,OAAQ,CAAE,EACVwF,OAAQ9E,IAGZmE,YAAa,qBACbY,SAAU,CAAC,OACXC,OAAQ,iBACRC,WAAY,uCACZC,QAAS,MACT1D,QAAQ,EACR2D,KAAM,CAAC"}