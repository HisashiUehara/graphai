{"version":3,"file":"bundle.cjs.min.js","sources":["../../llm_utils/lib/index.js","../src/openai_fetch_agent.ts"],"sourcesContent":["\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.getMessages = exports.getMergeValue = exports.flatString = void 0;\nconst flatString = (input) => {\n    return Array.isArray(input) ? input.filter((a) => a).join(\"\\n\") : (input ?? \"\");\n};\nexports.flatString = flatString;\nconst getMergeValue = (namedInputs, params, key, values) => {\n    const inputValue = namedInputs[key];\n    const paramsValue = params[key];\n    return inputValue || paramsValue ? [(0, exports.flatString)(inputValue), (0, exports.flatString)(paramsValue)].filter((a) => a).join(\"\\n\") : (0, exports.flatString)(values);\n};\nexports.getMergeValue = getMergeValue;\nconst getMessages = (systemPrompt, messages) => {\n    const messagesCopy = [...(systemPrompt ? [{ role: \"system\", content: systemPrompt }] : []), ...(messages ?? [])];\n    return messagesCopy;\n};\nexports.getMessages = getMessages;\n","import OpenAI from \"openai\";\nimport { AgentFunction, AgentFunctionInfo, sleep } from \"graphai\";\nimport { GraphAILLMInputBase, getMergeValue, getMessages } from \"@graphai/llm_utils\";\n\ntype OpenAIInputs = {\n  model?: string;\n  images?: string[];\n  tools?: OpenAI.ChatCompletionTool[];\n  tool_choice?: OpenAI.ChatCompletionToolChoiceOption;\n  max_tokens?: number;\n  verbose?: boolean;\n  temperature?: number;\n  // baseURL?: string;\n  apiKey?: string;\n  // stream?: boolean;\n  messages?: Array<OpenAI.ChatCompletionMessageParam>;\n  forWeb?: boolean;\n  response_format?: any;\n} & GraphAILLMInputBase;\n\nconst convertOpenAIChatCompletion = (response: OpenAI.ChatCompletion, messages: OpenAI.ChatCompletionMessageParam[]) => {\n  const message = response?.choices[0] && response?.choices[0].message ? response?.choices[0].message : null;\n  const text = message && message.content ? message.content : null;\n\n  const functionResponse = message?.tool_calls && message?.tool_calls[0] ? message?.tool_calls[0] : null;\n  // const functionId = message?.tool_calls && message?.tool_calls[0] ? message?.tool_calls[0]?.id : null;\n\n  const tool = functionResponse\n    ? {\n        id: functionResponse.id,\n        name: functionResponse?.function?.name,\n        arguments: (() => {\n          try {\n            return JSON.parse(functionResponse?.function?.arguments);\n          } catch (__e) {\n            return undefined;\n          }\n        })(),\n      }\n    : undefined;\n\n  if (message) {\n    messages.push(message);\n  }\n  return {\n    ...response,\n    text,\n    tool,\n    message,\n    messages,\n  };\n};\n\nexport const openAIFetchAgent: AgentFunction<OpenAIInputs, Record<string, any> | string, string | Array<any>, OpenAIInputs> = async ({\n  params,\n  namedInputs,\n}) => {\n  const { verbose, system, images, temperature, tools, tool_choice, max_tokens, /* baseURL, stream, */ apiKey, prompt, messages, response_format } = {\n    ...params,\n    ...namedInputs,\n  };\n\n  const userPrompt = getMergeValue(namedInputs, params, \"mergeablePrompts\", prompt);\n  const systemPrompt = getMergeValue(namedInputs, params, \"mergeableSystem\", system);\n\n  const messagesCopy = getMessages<OpenAI.ChatCompletionMessageParam>(systemPrompt, messages);\n\n  if (!apiKey) {\n    throw new Error(\"OPENAI_API_KEY key is not set in params. params: {apiKey: 'sk-xxx'}\");\n  }\n\n  if (userPrompt) {\n    messagesCopy.push({\n      role: \"user\",\n      content: userPrompt,\n    });\n  }\n  if (images) {\n    const image_url =\n      params.model === \"gpt-4-vision-preview\"\n        ? images[0]\n        : {\n            url: images[0],\n            detail: \"high\",\n          };\n    messagesCopy.push({\n      role: \"user\",\n      content: [\n        {\n          type: \"image_url\",\n          image_url,\n        } as OpenAI.ChatCompletionContentPart,\n      ],\n    });\n  }\n\n  if (verbose) {\n    console.log(messagesCopy);\n  }\n\n  const chatParams = {\n    model: params.model || \"gpt-4o\",\n    messages: messagesCopy as unknown as OpenAI.ChatCompletionMessageParam[],\n    tools,\n    tool_choice,\n    max_tokens,\n    temperature: temperature ?? 0.7,\n    response_format,\n  };\n\n  const response = await fetch(\"https://api.openai.com/v1/chat/completions\", {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n      Authorization: `Bearer ${apiKey}`,\n    },\n    body: JSON.stringify(chatParams),\n  });\n\n  if (response.status === 200) {\n    const result = await response.json();\n    return convertOpenAIChatCompletion(result, messagesCopy);\n  }\n  throw new Error(\"OPENAI API Error\");\n};\n\nconst input_sample = \"this is response result\";\nconst result_sample = {\n  object: \"chat.completion\",\n  id: \"chatcmpl-9N7HxXYbwjmdbdiQE94MHoVluQhyt\",\n  choices: [\n    {\n      message: {\n        role: \"assistant\",\n        content: input_sample,\n      },\n      finish_reason: \"stop\",\n      index: 0,\n      logprobs: null,\n    },\n  ],\n  created: 1715296589,\n  model: \"gpt-3.5-turbo-0125\",\n};\n\nexport const openAIMockAgent: AgentFunction<\n  {\n    model?: string;\n    query?: string;\n    system?: string;\n    verbose?: boolean;\n    temperature?: number;\n  },\n  Record<string, any> | string,\n  string | Array<any>\n> = async ({ filterParams }) => {\n  for await (const token of input_sample.split(\"\")) {\n    if (filterParams && filterParams.streamTokenCallback && token) {\n      await sleep(100);\n      filterParams.streamTokenCallback(token);\n    }\n  }\n\n  return result_sample;\n};\nconst openAIFetchAgentInfo: AgentFunctionInfo = {\n  name: \"openAIFetchAgent\",\n  agent: openAIFetchAgent,\n  mock: openAIMockAgent,\n  inputs: {\n    type: \"object\",\n    properties: {\n      model: { type: \"string\" },\n      system: { type: \"string\" },\n      tools: { type: \"object\" },\n      tool_choice: {\n        anyOf: [{ type: \"array\" }, { type: \"object\" }],\n      },\n      max_tokens: { type: \"number\" },\n      verbose: { type: \"boolean\" },\n      temperature: { type: \"number\" },\n      // baseURL: { type: \"string\" },\n      apiKey: {\n        anyOf: [{ type: \"string\" }, { type: \"object\" }],\n      },\n      // stream: { type: \"boolean\" },\n      prompt: {\n        type: \"string\",\n        description: \"query string\",\n      },\n      messages: {\n        anyOf: [{ type: \"string\" }, { type: \"object\" }, { type: \"array\" }],\n        description: \"chat messages\",\n      },\n    },\n  },\n  output: {\n    type: \"object\",\n    properties: {\n      id: {\n        type: \"string\",\n      },\n      object: {\n        type: \"string\",\n      },\n      created: {\n        type: \"integer\",\n      },\n      model: {\n        type: \"string\",\n      },\n      choices: {\n        type: \"array\",\n        items: [\n          {\n            type: \"object\",\n            properties: {\n              index: {\n                type: \"integer\",\n              },\n              message: {\n                type: \"array\",\n                items: [\n                  {\n                    type: \"object\",\n                    properties: {\n                      content: {\n                        type: \"string\",\n                      },\n                      role: {\n                        type: \"string\",\n                      },\n                    },\n                    required: [\"content\", \"role\"],\n                  },\n                ],\n              },\n            },\n            required: [\"index\", \"message\", \"logprobs\", \"finish_reason\"],\n          },\n        ],\n      },\n      usage: {\n        type: \"object\",\n        properties: {\n          prompt_tokens: {\n            type: \"integer\",\n          },\n          completion_tokens: {\n            type: \"integer\",\n          },\n          total_tokens: {\n            type: \"integer\",\n          },\n        },\n        required: [\"prompt_tokens\", \"completion_tokens\", \"total_tokens\"],\n      },\n      text: {\n        type: \"string\",\n      },\n      tool: {\n        arguments: {\n          type: \"object\",\n        },\n        name: {\n          type: \"string\",\n        },\n      },\n      message: {\n        type: \"object\",\n        properties: {\n          content: {\n            type: \"string\",\n          },\n          role: {\n            type: \"string\",\n          },\n        },\n        required: [\"content\", \"role\"],\n      },\n    },\n    required: [\"id\", \"object\", \"created\", \"model\", \"choices\", \"usage\"],\n  },\n  params: {\n    type: \"object\",\n    properties: {\n      model: { type: \"string\" },\n      system: { type: \"string\" },\n      tools: { type: \"object\" },\n      tool_choice: { anyOf: [{ type: \"array\" }, { type: \"object\" }] },\n      max_tokens: { type: \"number\" },\n      verbose: { type: \"boolean\" },\n      temperature: { type: \"number\" },\n      // baseURL: { type: \"string\" },\n      apiKey: { anyOf: [{ type: \"string\" }, { type: \"object\" }] },\n      // stream: { type: \"boolean\" },\n      prompt: { type: \"string\", description: \"query string\" },\n      messages: { anyOf: [{ type: \"string\" }, { type: \"object\" }, { type: \"array\" }], description: \"chat messages\" },\n    },\n  },\n  outputFormat: {\n    llmResponse: {\n      key: \"choices.$0.message.content\",\n      type: \"string\",\n    },\n  },\n  samples: [\n    {\n      inputs: { prompt: input_sample },\n      params: {},\n      result: result_sample,\n    },\n  ],\n  description: \"OpenAI Fetch Agent\",\n  category: [\"llm\"],\n  author: \"Receptron team\",\n  repository: \"https://github.com/receptron/graphai\",\n  license: \"MIT\",\n  stream: false,\n  npms: [\"openai\"],\n};\n\nexport default openAIFetchAgentInfo;\n"],"names":["Object","defineProperty","exports","value","getMessages","getMergeValue","flatString","input","Array","isArray","filter","a","join","namedInputs","params","key","values","inputValue","paramsValue","systemPrompt","messages","role","content","input_sample","result_sample","object","id","choices","message","finish_reason","index","logprobs","created","model","openAIFetchAgentInfo","name","agent","async","verbose","system","images","temperature","tools","tool_choice","max_tokens","apiKey","prompt","response_format","userPrompt","messagesCopy","Error","push","image_url","url","detail","type","console","log","chatParams","response","fetch","method","headers","Authorization","body","JSON","stringify","status","text","functionResponse","tool_calls","tool","function","arguments","parse","__e","undefined","convertOpenAIChatCompletion","json","mock","filterParams","token","split","streamTokenCallback","sleep","inputs","properties","anyOf","description","output","items","required","usage","prompt_tokens","completion_tokens","total_tokens","outputFormat","llmResponse","samples","result","category","author","repository","license","stream","npms"],"mappings":"wEACAA,OAAOC,eAAcC,EAAU,aAAc,CAAEC,OAAO,IACtDD,EAAAE,YAAsBF,EAAwBG,cAAAH,EAAAI,gBAAqB,EAInEJ,EAAAI,WAHoBC,GACTC,MAAMC,QAAQF,GAASA,EAAMG,QAAQC,GAAMA,IAAGC,KAAK,MAASL,GAAS,GAQhFL,EAAAG,cALsB,CAACQ,EAAaC,EAAQC,EAAKC,KAC7C,MAAMC,EAAaJ,EAAYE,GACzBG,EAAcJ,EAAOC,GAC3B,OAAOE,GAAcC,EAAc,EAAC,EAAIhB,EAAQI,YAAYW,IAAa,EAAIf,EAAQI,YAAYY,IAAcR,QAAQC,GAAMA,IAAGC,KAAK,OAAQ,EAAIV,EAAQI,YAAYU,EAAO,EAOhLd,EAAAE,YAJoB,CAACe,EAAcC,IACV,IAAKD,EAAe,CAAC,CAAEE,KAAM,SAAUC,QAASH,IAAkB,MAASC,GAAY,YCMhH,MA0GMG,EAAe,0BACfC,EAAgB,CACpBC,OAAQ,kBACRC,GAAI,yCACJC,QAAS,CACP,CACEC,QAAS,CACPP,KAAM,YACNC,QAASC,GAEXM,cAAe,OACfC,MAAO,EACPC,SAAU,OAGdC,QAAS,WACTC,MAAO,sBAuBHC,EAA0C,CAC9CC,KAAM,mBACNC,MAlH4HC,OAC5HvB,SACAD,kBAEA,MAAMyB,QAAEA,EAAOC,OAAEA,EAAMC,OAAEA,EAAMC,YAAEA,EAAWC,MAAEA,EAAKC,YAAEA,EAAWC,WAAEA,EAAUC,OAAyBA,EAAMC,OAAEA,EAAM1B,SAAEA,EAAQ2B,gBAAEA,GAAoB,IAC9IjC,KACAD,GAGCmC,EAAa3C,EAAAA,cAAcQ,EAAaC,EAAQ,mBAAoBgC,GACpE3B,EAAed,EAAAA,cAAcQ,EAAaC,EAAQ,kBAAmByB,GAErEU,EAAe7C,EAAAA,YAA+Ce,EAAcC,GAElF,IAAKyB,EACH,MAAM,IAAIK,MAAM,uEASlB,GANIF,GACFC,EAAaE,KAAK,CAChB9B,KAAM,OACNC,QAAS0B,IAGTR,EAAQ,CACV,MAAMY,EACa,yBAAjBtC,EAAOmB,MACHO,EAAO,GACP,CACEa,IAAKb,EAAO,GACZc,OAAQ,QAEhBL,EAAaE,KAAK,CAChB9B,KAAM,OACNC,QAAS,CACP,CACEiC,KAAM,YACNH,gBAMJd,GACFkB,QAAQC,IAAIR,GAGd,MAAMS,EAAa,CACjBzB,MAAOnB,EAAOmB,OAAS,SACvBb,SAAU6B,EACVP,QACAC,cACAC,aACAH,YAAaA,GAAe,GAC5BM,mBAGIY,QAAiBC,MAAM,6CAA8C,CACzEC,OAAQ,OACRC,QAAS,CACP,eAAgB,mBAChBC,cAAe,UAAUlB,KAE3BmB,KAAMC,KAAKC,UAAUR,KAGvB,GAAwB,MAApBC,EAASQ,OAAgB,CAE3B,MArGgC,EAACR,EAAiCvC,KACpE,MAAMQ,EAAU+B,GAAUhC,QAAQ,IAAMgC,GAAUhC,QAAQ,GAAGC,QAAU+B,GAAUhC,QAAQ,GAAGC,QAAU,KAChGwC,EAAOxC,GAAWA,EAAQN,QAAUM,EAAQN,QAAU,KAEtD+C,EAAmBzC,GAAS0C,YAAc1C,GAAS0C,WAAW,GAAK1C,GAAS0C,WAAW,GAAK,KAG5FC,EAAOF,EACT,CACE3C,GAAI2C,EAAiB3C,GACrBS,KAAMkC,GAAkBG,UAAUrC,KAClCsC,UAAW,MACT,IACE,OAAOR,KAAKS,MAAML,GAAkBG,UAAUC,WAC9C,MAAOE,GACP,OAEH,EANU,SAQbC,EAKJ,OAHIhD,GACFR,EAAS+B,KAAKvB,GAET,IACF+B,EACHS,OACAG,OACA3C,UACAR,WACD,EAuEQyD,OADclB,EAASmB,OACa7B,GAE7C,MAAM,IAAIC,MAAM,mBAAmB,EA6CnC6B,KAbE1C,OAAS2C,mBACX,UAAW,MAAMC,KAAS1D,EAAa2D,MAAM,IACvCF,GAAgBA,EAAaG,qBAAuBF,UAChDG,EAAAA,MAAM,KACZJ,EAAaG,oBAAoBF,IAIrC,OAAOzD,CAAa,EAMpB6D,OAAQ,CACN9B,KAAM,SACN+B,WAAY,CACVrD,MAAO,CAAEsB,KAAM,UACfhB,OAAQ,CAAEgB,KAAM,UAChBb,MAAO,CAAEa,KAAM,UACfZ,YAAa,CACX4C,MAAO,CAAC,CAAEhC,KAAM,SAAW,CAAEA,KAAM,YAErCX,WAAY,CAAEW,KAAM,UACpBjB,QAAS,CAAEiB,KAAM,WACjBd,YAAa,CAAEc,KAAM,UAErBV,OAAQ,CACN0C,MAAO,CAAC,CAAEhC,KAAM,UAAY,CAAEA,KAAM,YAGtCT,OAAQ,CACNS,KAAM,SACNiC,YAAa,gBAEfpE,SAAU,CACRmE,MAAO,CAAC,CAAEhC,KAAM,UAAY,CAAEA,KAAM,UAAY,CAAEA,KAAM,UACxDiC,YAAa,mBAInBC,OAAQ,CACNlC,KAAM,SACN+B,WAAY,CACV5D,GAAI,CACF6B,KAAM,UAER9B,OAAQ,CACN8B,KAAM,UAERvB,QAAS,CACPuB,KAAM,WAERtB,MAAO,CACLsB,KAAM,UAER5B,QAAS,CACP4B,KAAM,QACNmC,MAAO,CACL,CACEnC,KAAM,SACN+B,WAAY,CACVxD,MAAO,CACLyB,KAAM,WAER3B,QAAS,CACP2B,KAAM,QACNmC,MAAO,CACL,CACEnC,KAAM,SACN+B,WAAY,CACVhE,QAAS,CACPiC,KAAM,UAERlC,KAAM,CACJkC,KAAM,WAGVoC,SAAU,CAAC,UAAW,YAK9BA,SAAU,CAAC,QAAS,UAAW,WAAY,oBAIjDC,MAAO,CACLrC,KAAM,SACN+B,WAAY,CACVO,cAAe,CACbtC,KAAM,WAERuC,kBAAmB,CACjBvC,KAAM,WAERwC,aAAc,CACZxC,KAAM,YAGVoC,SAAU,CAAC,gBAAiB,oBAAqB,iBAEnDvB,KAAM,CACJb,KAAM,UAERgB,KAAM,CACJE,UAAW,CACTlB,KAAM,UAERpB,KAAM,CACJoB,KAAM,WAGV3B,QAAS,CACP2B,KAAM,SACN+B,WAAY,CACVhE,QAAS,CACPiC,KAAM,UAERlC,KAAM,CACJkC,KAAM,WAGVoC,SAAU,CAAC,UAAW,UAG1BA,SAAU,CAAC,KAAM,SAAU,UAAW,QAAS,UAAW,UAE5D7E,OAAQ,CACNyC,KAAM,SACN+B,WAAY,CACVrD,MAAO,CAAEsB,KAAM,UACfhB,OAAQ,CAAEgB,KAAM,UAChBb,MAAO,CAAEa,KAAM,UACfZ,YAAa,CAAE4C,MAAO,CAAC,CAAEhC,KAAM,SAAW,CAAEA,KAAM,YAClDX,WAAY,CAAEW,KAAM,UACpBjB,QAAS,CAAEiB,KAAM,WACjBd,YAAa,CAAEc,KAAM,UAErBV,OAAQ,CAAE0C,MAAO,CAAC,CAAEhC,KAAM,UAAY,CAAEA,KAAM,YAE9CT,OAAQ,CAAES,KAAM,SAAUiC,YAAa,gBACvCpE,SAAU,CAAEmE,MAAO,CAAC,CAAEhC,KAAM,UAAY,CAAEA,KAAM,UAAY,CAAEA,KAAM,UAAYiC,YAAa,mBAGjGQ,aAAc,CACZC,YAAa,CACXlF,IAAK,6BACLwC,KAAM,WAGV2C,QAAS,CACP,CACEb,OAAQ,CAAEvC,OAAQvB,GAClBT,OAAQ,CAAE,EACVqF,OAAQ3E,IAGZgE,YAAa,qBACbY,SAAU,CAAC,OACXC,OAAQ,iBACRC,WAAY,uCACZC,QAAS,MACTC,QAAQ,EACRC,KAAM,CAAC"}